"""
Test de performance pour mesurer les gains de la vectorisation par timestamp.
Compare l'ancien traitement ligne par ligne vs le nouveau traitement group√©.
"""

import time
import pandas as pd
import numpy as np
from pathlib import Path

print("="*70)
print("TEST D'OPTIMISATION - VECTORISATION PAR TIMESTAMP")
print("="*70)

# Simuler un DataFrame de stations avec timestamps r√©p√©t√©s
print("\nüìä G√©n√©ration de donn√©es de test...")

# Cr√©er 1000 relev√©s avec 50 timestamps uniques (en moyenne 20 stations par timestamp)
np.random.seed(42)
n_samples = 1000
n_unique_timestamps = 50
n_stations = 100

timestamps = pd.date_range('2016-01-01', periods=n_unique_timestamps, freq='3H')
data = {
    'datetime': np.random.choice(timestamps, n_samples),
    'number_sta': np.random.choice(range(1000, 1000 + n_stations), n_samples),
    'lat': np.random.uniform(45, 47, n_samples),
    'lon': np.random.uniform(4, 6, n_samples),
    't': np.random.uniform(270, 290, n_samples),
    'hu': np.random.uniform(50, 100, n_samples),
}

stations_df = pd.DataFrame(data)
stations_df = stations_df.sort_values(['datetime', 'number_sta'])

print(f"   ‚úì {len(stations_df)} relev√©s g√©n√©r√©s")
print(f"   ‚úì {stations_df['datetime'].nunique()} timestamps uniques")
print(f"   ‚úì {stations_df['number_sta'].nunique()} stations uniques")
print(f"   ‚úì Moyenne: {len(stations_df) / stations_df['datetime'].nunique():.1f} stations par timestamp")

# Simuler le chargement d'images (avec d√©lai artificiel)
def simulate_image_loading(timestamp):
    """Simule le temps de chargement d'un ensemble d'images satellites"""
    # Simuler 1ms de traitement (I/O, d√©compression, etc.)
    time.sleep(0.001)
    return {
        -12: {'IR108': np.random.rand(171, 261)},
        -24: {'IR108': np.random.rand(171, 261)},
    }

# TEST 1: M√©thode ANCIENNE (ligne par ligne)
print("\n" + "="*70)
print("TEST 1: M√©thode ANCIENNE (ligne par ligne)")
print("="*70)

start_old = time.time()
samples_old = []
image_loads_old = 0

for _, row in stations_df.iterrows():
    ref_time = pd.Timestamp(row['datetime'])
    
    # Charger les images (simul√©)
    multi_images = simulate_image_loading(ref_time)
    image_loads_old += 1
    
    # Cr√©er le sample
    sample = {
        'timestamp': ref_time,
        'station_id': int(row['number_sta']),
        'images': multi_images,
        't': row['t'],
        'hu': row['hu'],
    }
    samples_old.append(sample)

end_old = time.time()
elapsed_old = end_old - start_old

print(f"‚è±Ô∏è  Temps total: {elapsed_old:.3f}s")
print(f"üì¶ Samples cr√©√©s: {len(samples_old)}")
print(f"üîÑ Chargements d'images: {image_loads_old}")
print(f"‚è±Ô∏è  Temps moyen par sample: {elapsed_old/len(samples_old)*1000:.2f}ms")

# TEST 2: M√©thode NOUVELLE (vectoris√©e par timestamp)
print("\n" + "="*70)
print("TEST 2: M√©thode NOUVELLE (vectorisation par timestamp)")
print("="*70)

start_new = time.time()
samples_new = []
image_loads_new = 0
image_cache = {}

# Grouper par timestamp
grouped = stations_df.groupby('datetime')
print(f"   Groupes cr√©√©s: {len(grouped)}")

for ref_time, group_df in grouped:
    ref_time = pd.Timestamp(ref_time)
    
    # V√©rifier le cache
    cache_key = f"multi_{ref_time}"
    if cache_key in image_cache:
        multi_images = image_cache[cache_key]
    else:
        # Charger les images UNE SEULE FOIS
        multi_images = simulate_image_loading(ref_time)
        image_cache[cache_key] = multi_images
        image_loads_new += 1
    
    # Traiter TOUTES les stations de ce timestamp
    for _, row in group_df.iterrows():
        sample = {
            'timestamp': ref_time,
            'station_id': int(row['number_sta']),
            'images': multi_images,  # R√âUTILISATION
            't': row['t'],
            'hu': row['hu'],
        }
        samples_new.append(sample)

end_new = time.time()
elapsed_new = end_new - start_new

print(f"‚è±Ô∏è  Temps total: {elapsed_new:.3f}s")
print(f"üì¶ Samples cr√©√©s: {len(samples_new)}")
print(f"üîÑ Chargements d'images: {image_loads_new}")
print(f"‚è±Ô∏è  Temps moyen par sample: {elapsed_new/len(samples_new)*1000:.2f}ms")

# COMPARAISON
print("\n" + "="*70)
print("üìä R√âSULTATS")
print("="*70)

speedup = elapsed_old / elapsed_new
time_saved = elapsed_old - elapsed_new
reduction_io = (image_loads_old - image_loads_new) / image_loads_old * 100

print(f"\nüöÄ Gain de performance:")
print(f"   ‚Ä¢ M√©thode ancienne: {elapsed_old:.3f}s ({image_loads_old} chargements)")
print(f"   ‚Ä¢ M√©thode nouvelle: {elapsed_new:.3f}s ({image_loads_new} chargements)")
print(f"   ‚Ä¢ Acc√©l√©ration: {speedup:.1f}x plus rapide")
print(f"   ‚Ä¢ Temps √©conomis√©: {time_saved:.3f}s")
print(f"   ‚Ä¢ R√©duction I/O: -{reduction_io:.1f}% de chargements")

# Validation
print(f"\n‚úÖ Validation:")
print(f"   ‚Ä¢ Nombre de samples: {len(samples_old)} vs {len(samples_new)} {'‚úì' if len(samples_old) == len(samples_new) else '‚úó'}")

# Projection pour 83,000 samples
print(f"\nüéØ PROJECTION POUR 83,000 SAMPLES:")

# Estimer le ratio timestamps uniques / total samples (ici ~20 stations/timestamp)
ratio = len(stations_df) / stations_df['datetime'].nunique()
estimated_unique_timestamps = int(83000 / ratio)

print(f"   ‚Ä¢ Ratio moyen: {ratio:.1f} stations par timestamp")
print(f"   ‚Ä¢ Timestamps uniques estim√©s: {estimated_unique_timestamps:,}")

projected_old = (elapsed_old / len(samples_old)) * 83000
projected_new = (elapsed_new / len(samples_new)) * 83000

print(f"   ‚Ä¢ M√©thode ancienne: {projected_old:.1f}s ({projected_old/60:.1f}min)")
print(f"   ‚Ä¢ M√©thode nouvelle: {projected_new:.1f}s ({projected_new/60:.1f}min)")
print(f"   ‚Ä¢ Temps √©conomis√©: {(projected_old - projected_new):.1f}s ({(projected_old - projected_new)/60:.1f}min)")

# Impact combin√© avec √©tape 1
print(f"\nüéØ GAIN CUMUL√â (√âtape 1 + √âtape 2):")
print(f"   ‚Ä¢ √âtape 1 (pr√©-indexation): 8-71x")
print(f"   ‚Ä¢ √âtape 2 (vectorisation): {speedup:.1f}x")
print(f"   ‚Ä¢ Gain cumul√© estim√©: {8 * speedup:.0f}-{71 * speedup:.0f}x")

print("\n" + "="*70)
print("‚úÖ TEST TERMIN√â")
print("="*70)
